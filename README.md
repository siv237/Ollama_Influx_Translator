# Ollama to InfluxDB Log Translator

Этот проект представляет собой системную службу (транслятор), которая непрерывно собирает логи из `systemd-journal` для сервиса `Ollama` и отправляет их в базу данных InfluxDB. Он идеально подходит для централизованного мониторинга и анализа работы моделей Ollama на одном или нескольких серверах.

## Ключевые возможности

- **Централизованный сбор логов**: Собирайте логи с нескольких хостов в одну базу InfluxDB. Данные автоматически тегируются по имени хоста.
- **Работа в реальном времени**: Служба проверяет наличие новых логов каждые **10 секунд**.
- **Надежная доставка**: Встроенный механизм дедупликации на основе хеша предотвращает запись дубликатов.
- **Инвентаризация моделей**: Автоматически сканирует и обновляет список доступных моделей Ollama каждый час.
- **Безопасность**: Работает от имени пользователя с ограниченными правами, получая доступ к логам через безопасное правило `sudoers`.
- **Простая установка**: Установочный скрипт автоматически настраивает окружение, зависимости и системную службу `systemd`.

## Как это работает

Транслятор работает как постоянная фоновая служба, следуя простой, но эффективной логике:

1.  **Первый запуск**: При первом старте служба запрашивает все логи Ollama за последние **24 часа**, чтобы обеспечить базовый контекст для анализа.
2.  **Непрерывный мониторинг**: Каждые 10 секунд служба запрашивает только те логи, которые появились с момента последней успешной проверки.
3.  **Дедупликация**: Перед отправкой в InfluxDB каждая запись лога хешируется. Этот хеш используется как тег `msg_hash`, что позволяет избежать дублирования данных на стороне базы данных.
4.  **Инвентаризация моделей**: При запуске и далее каждый час служба сканирует директорию с моделями Ollama и записывает их список в отдельное измерение (`ollama_models_inventory`) в InfluxDB. Это позволяет отслеживать, какие модели доступны на каждом хосте.

## Установка

1.  **Клонируйте репозиторий** на сервер, где работает Ollama:
    ```bash
    git clone https://github.com/siv237/Ollama_Influx_Translator.git
    cd Ollama_Influx_Translator
    ```

2.  **Настройте подключение к InfluxDB**:
    Скопируйте файл `config.env.example` в `config.env` и укажите ваши учетные данные.
    ```bash
    cp config.env.example config.env
    nano config.env
    ```
    Содержимое `config.env`:
    ```ini
    INFLUXDB_URL="http://your-influxdb-server:8086"
    INFLUXDB_TOKEN="your-influxdb-token"
    INFLUXDB_ORG="your-organization"
    INFLUXDB_BUCKET="ollama-logs"
    ```

3.  **Запустите установочный скрипт**:
    Скрипт должен выполняться с правами `sudo`, так как он создает системную службу и настраивает права доступа.
    ```bash
    sudo ./install.sh
    ```

Скрипт автоматически определит пользователя, от которого работает служба `ollama`, и настроит для него безопасный доступ к логам через `sudoers`.

## Управление службой

- **Запустить службу**:
  ```bash
  sudo systemctl start Ollama_InfluxDB
  ```
- **Проверить статус**:
  ```bash
  sudo systemctl status Ollama_InfluxDB
  ```
- **Смотреть логи в реальном времени**:
  ```bash
  journalctl -u Ollama_InfluxDB -f
  ```
- **Остановить службу**:
  ```bash
  sudo systemctl stop Ollama_InfluxDB
  ```

## Структура данных в InfluxDB

Данные сохраняются в два измерения (measurements):

1.  **`ollama_logs`** (основные логи)
    - **Tags**:
        - `host`: Имя хоста, с которого пришли логи.
        - `source`: Источник логов (всегда `systemd`).
        - `msg_hash`: Уникальный хеш для дедупликации.
    - **Fields**:
        - `message`: Полный текст лога.

2.  **`ollama_models_inventory`** (инвентарь моделей)
    - **Tags**:
        - `host`: Имя хоста.
    - **Fields**:
        - `models_json`: Список моделей в формате JSON-строки.

## Требования

- Python 3.7+
- InfluxDB 2.0+
- Ollama (установленный как служба `systemd`)
